---
phase: 06-voice
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - frontend/src/hooks/useVoice.ts
  - frontend/src/lib/audioUtils.ts
autonomous: true

must_haves:
  truths:
    - "Frontend can capture microphone audio"
    - "Frontend can play audio from WebSocket stream"
    - "Frontend maintains voice WebSocket connection"
  artifacts:
    - path: "frontend/src/hooks/useVoice.ts"
      provides: "Voice state management and audio I/O"
      min_lines: 100
      exports: ["useVoice"]
    - path: "frontend/src/lib/audioUtils.ts"
      provides: "Audio encoding/decoding utilities"
      min_lines: 30
      exports: ["pcm16ToFloat32", "float32ToPcm16Base64"]
  key_links:
    - from: "useVoice.ts"
      to: "ws://localhost:8000/voice"
      via: "WebSocket connection"
      pattern: "new WebSocket.*voice"
    - from: "useVoice.ts"
      to: "navigator.mediaDevices.getUserMedia"
      via: "mic capture"
      pattern: "getUserMedia"
---

<objective>
Create frontend voice capture and audio playback infrastructure.

Purpose: Implement the browser-side audio handling for voice interactions - microphone capture, encoding to PCM16, WebSocket streaming to backend, and playback of AI voice responses.

Output: `frontend/src/hooks/useVoice.ts` hook for voice state and audio I/O, `frontend/src/lib/audioUtils.ts` for audio format conversion.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/06-voice/06-CONTEXT.md

# Pattern reference - existing WebSocket hook
@frontend/src/hooks/useWebSocket.ts
@frontend/src/lib/websocket.ts

# App structure for integration context
@frontend/src/App.tsx
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create audio utility functions</name>
  <files>frontend/src/lib/audioUtils.ts</files>
  <action>
Create `frontend/src/lib/audioUtils.ts` with audio format conversion utilities.

**gpt-realtime requires PCM16 format:**
- 16-bit signed integers
- Little-endian byte order
- 24000 Hz sample rate
- Mono channel

**Functions needed:**

```typescript
/**
 * Convert PCM16 base64 audio to Float32Array for Web Audio API playback.
 * gpt-realtime sends audio as base64-encoded PCM16.
 */
export function pcm16ToFloat32(base64: string): Float32Array {
  // 1. Decode base64 to bytes
  // 2. Create Int16Array view
  // 3. Convert to Float32 (-1.0 to 1.0 range)
  // PCM16 range: -32768 to 32767
}

/**
 * Convert Float32Array audio (from mic) to base64-encoded PCM16.
 * Browser MediaRecorder gives Float32, gpt-realtime needs PCM16.
 */
export function float32ToPcm16Base64(float32: Float32Array): string {
  // 1. Convert Float32 (-1.0 to 1.0) to Int16 (-32768 to 32767)
  // 2. Create Uint8Array of bytes (little-endian)
  // 3. Encode to base64
}

/**
 * Sample rate for gpt-realtime audio.
 */
export const VOICE_SAMPLE_RATE = 24000;
```

**Important:** Use proper clamping when converting between formats to avoid audio clipping.
  </action>
  <verify>
    - TypeScript compiles: `cd frontend && npx tsc --noEmit`
    - Functions exported correctly
  </verify>
  <done>
    - audioUtils.ts exists with pcm16ToFloat32 and float32ToPcm16Base64
    - VOICE_SAMPLE_RATE constant exported
  </done>
</task>

<task type="auto">
  <name>Task 2: Create useVoice hook</name>
  <files>frontend/src/hooks/useVoice.ts</files>
  <action>
Create `frontend/src/hooks/useVoice.ts` hook for voice state management and audio I/O.

**Interface:**

```typescript
interface UseVoiceOptions {
  onTranscript?: (role: 'user' | 'assistant', text: string) => void
  onToolResult?: (tool: string, result: Record<string, unknown>) => void
  onError?: (error: string) => void
}

interface UseVoiceReturn {
  // State
  isEnabled: boolean        // Voice mode active
  isMuted: boolean          // Mic muted
  isListening: boolean      // Currently detecting speech
  isSpeaking: boolean       // AI currently speaking
  status: 'disconnected' | 'connecting' | 'connected' | 'error'

  // Actions
  enable: () => Promise<void>   // Start voice mode
  disable: () => void           // Stop voice mode
  toggleMute: () => void        // Toggle mic mute
}
```

**Implementation details:**

1. **WebSocket connection:**
   - Connect to `ws://localhost:8000/voice` when enabled
   - Handle reconnection on disconnect (with backoff, same pattern as useWebSocket)
   - Close cleanly on disable

2. **Microphone capture:**
   - Use `navigator.mediaDevices.getUserMedia({ audio: true })`
   - Create AudioContext with 24000 Hz sample rate
   - Use ScriptProcessorNode or AudioWorklet to get raw audio samples
   - Convert to PCM16 and send via WebSocket: `{type: "audio", data: "<base64>"}`
   - When muted: stop sending audio but keep connection alive

3. **Audio playback:**
   - Create separate AudioContext for playback (24000 Hz)
   - Queue incoming audio chunks
   - Use AudioBufferSourceNode for smooth playback
   - Decode PCM16 from base64 using audioUtils

4. **WebSocket message handling:**
   ```typescript
   // Incoming message types from backend:
   {type: "status", status: "connected"|"error", ...}
   {type: "speech_started"}  // User started speaking
   {type: "speech_stopped"}  // User stopped speaking
   {type: "audio", data: "<base64-pcm16>"}  // AI audio chunk
   {type: "transcript", role: "user"|"assistant", text: "..."}
   {type: "tool_result", tool: "...", result: {...}}
   {type: "error", error: "..."}
   ```

5. **State management:**
   - `isListening` = true between speech_started and speech_stopped
   - `isSpeaking` = true while receiving audio chunks, false on silence
   - Track mute state locally (don't disconnect, just stop sending audio)

6. **Cleanup:**
   - Stop MediaStream tracks on disable
   - Close AudioContexts
   - Close WebSocket

**Audio capture approach (ScriptProcessorNode):**
```typescript
// Simpler than AudioWorklet, sufficient for demo
const source = audioContext.createMediaStreamSource(stream)
const processor = audioContext.createScriptProcessor(4096, 1, 1)
processor.onaudioprocess = (e) => {
  if (!muted && ws.readyState === WebSocket.OPEN) {
    const pcm16 = float32ToPcm16Base64(e.inputBuffer.getChannelData(0))
    ws.send(JSON.stringify({ type: 'audio', data: pcm16 }))
  }
}
source.connect(processor)
processor.connect(audioContext.destination) // Required for onaudioprocess to fire
```

**Note:** ScriptProcessorNode is deprecated but widely supported and simpler for MVP. AudioWorklet would be production-ready.
  </action>
  <verify>
    - TypeScript compiles: `cd frontend && npx tsc --noEmit`
    - Hook exports correctly
  </verify>
  <done>
    - useVoice hook exists with enable/disable/toggleMute
    - Handles mic capture and audio playback
    - Manages WebSocket connection to /voice
    - Exposes isEnabled, isMuted, isListening, isSpeaking, status
  </done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] `cd frontend && npx tsc --noEmit` passes
- [ ] audioUtils.ts exports pcm16ToFloat32, float32ToPcm16Base64, VOICE_SAMPLE_RATE
- [ ] useVoice.ts exports useVoice hook
- [ ] No TypeScript errors
</verification>

<success_criteria>
- All tasks completed
- audioUtils.ts provides PCM16 â†” Float32 conversion
- useVoice hook manages:
  - WebSocket connection to /voice endpoint
  - Microphone capture with mute support
  - Audio playback from AI responses
  - State for UI (isEnabled, isMuted, isListening, isSpeaking)
- Callbacks for transcript and tool results
</success_criteria>

<output>
After completion, create `.planning/phases/06-voice/06-02-SUMMARY.md`
</output>
