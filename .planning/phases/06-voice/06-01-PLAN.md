---
phase: 06-voice
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - backend/voice.py
  - backend/main.py
autonomous: true

must_haves:
  truths:
    - "Backend can connect to gpt-realtime WebSocket API"
    - "Backend can send audio input and receive audio output"
    - "Backend can handle tool calls from voice model"
  artifacts:
    - path: "backend/voice.py"
      provides: "gpt-realtime WebSocket handler with tool calling"
      min_lines: 150
      exports: ["handle_voice_session"]
  key_links:
    - from: "backend/voice.py"
      to: "gpt-realtime API"
      via: "websockets library connection"
      pattern: "websockets\\.connect"
    - from: "backend/voice.py"
      to: "backend/tools.py"
      via: "execute_tool import and call"
      pattern: "execute_tool"
---

<objective>
Create backend infrastructure for gpt-realtime voice integration.

Purpose: Establish the server-side foundation for voice interactions using Azure's gpt-realtime API. This handles the authenticated WebSocket connection to the realtime model, audio streaming, session management, and tool calling integration.

Output: `backend/voice.py` with gpt-realtime handler, updated `backend/main.py` with `/voice` WebSocket endpoint.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/06-voice/06-CONTEXT.md

# Prior phase summaries for established patterns
@.planning/phases/05-persona/05-01-SUMMARY.md

# Existing code to understand patterns and integrate with
@backend/main.py
@backend/chat.py
@backend/auth.py
@backend/tools.py
@backend/modes.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create voice.py with gpt-realtime handler</name>
  <files>backend/voice.py</files>
  <action>
Create `backend/voice.py` with the gpt-realtime WebSocket handler.

**Architecture:**
- Bidirectional WebSocket relay: Browser WebSocket → Backend → gpt-realtime API
- Backend maintains authenticated connection to gpt-realtime
- Uses `websockets` library for async WebSocket client to Azure

**Implementation details:**

1. **Connection setup:**
   - Build URI: `wss://{AZURE_ENDPOINT}/openai/realtime?api-version=2025-04-01-preview&deployment=gpt-realtime`
   - Auth: Use `Authorization: Bearer {token}` header from azure.identity
   - Get token via DefaultAzureCredential (same pattern as auth.py)

2. **Session configuration:**
   - Send `session.update` on connect with:
     - `modalities: ["text", "audio"]`
     - `voice: "alloy"` (natural sounding)
     - `input_audio_format: "pcm16"` (browser standard)
     - `output_audio_format: "pcm16"`
     - `input_audio_transcription: { model: "whisper-1" }` (for displaying what user said)
     - `turn_detection: { type: "server_vad", threshold: 0.5, prefix_padding_ms: 300, silence_duration_ms: 500 }`
     - `tools: []` - reuse TOOL_DEFINITIONS from tools.py (show_chart, show_metrics)
     - `instructions: ""` - will be set dynamically based on current mode

3. **Event handling:**
   - `session.created` → log, session ready
   - `input_audio_buffer.speech_started` → forward to browser (for UI indicator)
   - `input_audio_buffer.speech_stopped` → forward to browser
   - `response.audio.delta` → forward base64 audio to browser
   - `response.audio.done` → forward completion signal
   - `response.audio_transcript.delta` → forward transcript for display
   - `response.function_call_arguments.done` → execute tool, send result back
   - `error` → log and forward to browser

4. **Tool calling flow:**
   - When `response.function_call_arguments.done` received:
     - Parse arguments JSON
     - Call `execute_tool(name, arguments)` from tools.py
     - Send `conversation.item.create` with function_call_output
     - Send `response.create` to continue (model needs explicit trigger after tool)
   - Forward tool_result to browser (same format as chat: `{type: "tool_result", ...}`)

5. **Session management:**
   - Store session state: current mode, persona (reuse from chat.py)
   - On mode switch command detected in transcript: update session instructions
   - Clean disconnect: close gpt-realtime WebSocket, then close browser WebSocket

**Function signature:**
```python
async def handle_voice_session(websocket: WebSocket) -> None:
    """
    Handle a voice session with bidirectional audio streaming.

    Browser WebSocket messages:
    - {"type": "audio", "data": "<base64-pcm16>"}
    - {"type": "mute", "muted": true/false}
    - {"type": "stop"}

    Messages sent to browser:
    - {"type": "status", "status": "connected"|"disconnected"|"error"}
    - {"type": "speech_started"}
    - {"type": "speech_stopped"}
    - {"type": "audio", "data": "<base64-pcm16>"}
    - {"type": "transcript", "role": "user"|"assistant", "text": "..."}
    - {"type": "tool_result", "tool": "...", "result": {...}}
    - {"type": "error", "error": "..."}
    """
```

**Dependencies to add:** `websockets` library (for async WebSocket client)

**Important:** Do NOT use the openai library's realtime client - it's not compatible with Azure. Use raw websockets with Azure auth.
  </action>
  <verify>
    - Python syntax check: `python -m py_compile backend/voice.py`
    - Imports resolve (no import errors when loading module)
  </verify>
  <done>
    - voice.py exists with handle_voice_session function
    - Handles session.update, audio streaming, tool calling
    - Uses Azure auth pattern from auth.py
  </done>
</task>

<task type="auto">
  <name>Task 2: Add /voice WebSocket endpoint to main.py</name>
  <files>backend/main.py</files>
  <action>
Add a new WebSocket endpoint `/voice` to main.py that routes to the voice handler.

**Changes:**
1. Import: `from voice import handle_voice_session`
2. Add WebSocket route:
   ```python
   @app.websocket("/voice")
   async def voice_endpoint(websocket: WebSocket):
       """WebSocket endpoint for voice interactions with gpt-realtime."""
       await websocket.accept()
       logger.info("Voice WebSocket connection established")
       try:
           await handle_voice_session(websocket)
       except WebSocketDisconnect:
           logger.info("Voice WebSocket connection closed")
       except Exception as e:
           logger.error(f"Voice WebSocket error: {e}")
   ```

3. Update startup log to mention voice endpoint:
   `logger.info("Voice WebSocket endpoint available at ws://localhost:8000/voice")`

**Note:** Keep existing `/ws` endpoint for text chat - they serve different purposes.
  </action>
  <verify>
    - `python -m py_compile backend/main.py`
    - Server starts without import errors
  </verify>
  <done>
    - /voice WebSocket endpoint added
    - Routes to handle_voice_session
    - Startup log mentions voice endpoint
  </done>
</task>

<task type="auto">
  <name>Task 3: Add websockets dependency</name>
  <files>backend/requirements.txt</files>
  <action>
Add `websockets` library to requirements.txt for the async WebSocket client.

Add line: `websockets>=12.0`

Then install: `pip install websockets` (in venv)
  </action>
  <verify>
    - `pip show websockets` shows installed version
    - `python -c "import websockets"` succeeds
  </verify>
  <done>
    - websockets added to requirements.txt
    - Package installed in venv
  </done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] `python -m py_compile backend/voice.py` passes
- [ ] `python -m py_compile backend/main.py` passes
- [ ] Server starts: `cd backend && python main.py` (will need browser auth)
- [ ] `/voice` endpoint listed in startup logs
</verification>

<success_criteria>
- All tasks completed
- voice.py implements gpt-realtime handler with:
  - Azure WebSocket connection setup
  - Session configuration with VAD and tools
  - Bidirectional audio streaming
  - Tool call execution and result forwarding
- main.py has /voice WebSocket endpoint
- websockets dependency installed
</success_criteria>

<output>
After completion, create `.planning/phases/06-voice/06-01-SUMMARY.md`
</output>
