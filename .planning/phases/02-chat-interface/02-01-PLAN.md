---
phase: 02-chat-interface
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - backend/chat.py
  - backend/main.py
autonomous: true

must_haves:
  truths:
    - "Backend receives chat messages via WebSocket"
    - "Backend calls Azure LLM with chat message"
    - "Backend streams LLM response chunks back to frontend"
  artifacts:
    - path: "backend/chat.py"
      provides: "LLM chat handler with streaming"
      exports: ["handle_chat_message"]
    - path: "backend/main.py"
      provides: "WebSocket message routing by type"
      contains: "handle_chat_message"
  key_links:
    - from: "main.py websocket_endpoint"
      to: "chat.handle_chat_message"
      via: "message type routing"
      pattern: "handle_chat_message"
    - from: "chat.py"
      to: "auth.get_inference_client"
      via: "Azure LLM call"
      pattern: "get_inference_client"
---

<objective>
Create backend chat handler with Azure LLM integration and streaming responses.

Purpose: Enable AI-powered chat by routing WebSocket messages to Azure's model-router deployment and streaming responses back to the frontend.

Output: Backend that receives chat messages, calls LLM, and streams response chunks via WebSocket.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-foundation/01-02-SUMMARY.md

Existing source files:
@backend/main.py
@backend/auth.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create chat handler with LLM streaming</name>
  <files>backend/chat.py</files>
  <action>
Create `backend/chat.py` that handles chat messages and streams LLM responses:

1. Import `get_inference_client` from auth.py
2. Create async generator `handle_chat_message(text: str, websocket: WebSocket)`:
   - Get inference client using existing auth module
   - Call `client.complete()` with model="model-router" (the deployment name from PROJECT.md)
   - Use streaming=True to get response chunks
   - For each chunk, yield/send via websocket with format: `{"type": "chat_chunk", "payload": {"text": chunk, "done": false}}`
   - When complete, send: `{"type": "chat_chunk", "payload": {"text": "", "done": true}}`
3. Handle errors gracefully - send error message if LLM call fails
4. Use simple system prompt: "You are a helpful AI assistant for Presto-Change-O, an industry simulation dashboard."

**Important:**
- Use `azure.ai.inference` ChatCompletionsClient (already in auth.py)
- The model deployment is "model-router" per PROJECT.md
- Send a "chat_start" message before streaming begins so frontend knows to show typing indicator
- Message format: `{"role": "user", "content": text}` for the API call
  </action>
  <verify>python -c "from chat import handle_chat_message; print('Import OK')" in backend directory</verify>
  <done>chat.py exports handle_chat_message that accepts text and websocket, streams response chunks</done>
</task>

<task type="auto">
  <name>Task 2: Update WebSocket routing for chat messages</name>
  <files>backend/main.py</files>
  <action>
Update `backend/main.py` to route messages by type:

1. Import `handle_chat_message` from chat.py
2. Update websocket_endpoint to:
   - Parse incoming JSON messages
   - Check message type
   - If type=="chat": extract text from payload, call handle_chat_message
   - Keep echo behavior for unknown types (for debugging)
3. Handle JSON parsing errors gracefully (send error response)
4. Add import for json module if not present

Message routing pattern:
```python
data = await websocket.receive_text()
message = json.loads(data)
if message.get("type") == "chat":
    text = message.get("payload", {}).get("text", "")
    await handle_chat_message(text, websocket)
else:
    # Echo for unknown types
    await websocket.send_text(f"Echo: {data}")
```

**Important:**
- Keep existing CORS and health check unchanged
- Keep lifespan handler unchanged
- Just update the websocket_endpoint function
  </action>
  <verify>python -c "from main import app; print('Import OK')" in backend directory</verify>
  <done>WebSocket endpoint routes chat messages to handler, other message types still echo</done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] `python -c "from main import app"` succeeds
- [ ] `python -c "from chat import handle_chat_message"` succeeds
- [ ] chat.py exists with handle_chat_message function
- [ ] main.py imports and uses handle_chat_message
- [ ] No syntax errors in either file
</verification>

<success_criteria>
- All tasks completed
- All verification checks pass
- Backend can receive chat messages and route to LLM handler
- Streaming response format ready for frontend consumption
</success_criteria>

<output>
After completion, create `.planning/phases/02-chat-interface/02-01-SUMMARY.md`
</output>
