---
phase: 01-foundation
plan: 02
type: execute
wave: 1
depends_on: []
files_modified: [backend/main.py, backend/requirements.txt, backend/auth.py, .env.example]
autonomous: true

must_haves:
  truths:
    - "User can run uvicorn and see FastAPI server running"
    - "Backend accepts WebSocket connections on /ws endpoint"
    - "Azure auth module can obtain token with InteractiveBrowserCredential"
  artifacts:
    - path: "backend/main.py"
      provides: "FastAPI application with WebSocket endpoint"
      contains: "websocket"
      min_lines: 30
    - path: "backend/requirements.txt"
      provides: "Python dependencies"
      contains: "fastapi"
    - path: "backend/auth.py"
      provides: "Azure authentication utilities"
      exports: ["get_azure_credential", "get_token"]
    - path: ".env.example"
      provides: "Environment variable template"
      contains: "AZURE"
  key_links:
    - from: "backend/main.py"
      to: "backend/auth.py"
      via: "import for credential access"
      pattern: "from auth import"
---

<objective>
Create Python FastAPI backend with WebSocket endpoint and Azure authentication.

Purpose: Establish the backend foundation that will host AI orchestration, mode switching, and voice handling in later phases.
Output: Working FastAPI server with WebSocket endpoint and Azure credential flow.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md

Azure Foundry Resources (from PROJECT.md):
- Project: aipmaker-project
- Endpoint: https://aipmaker-project-resource.services.ai.azure.com/api/projects/aipmaker-project
- Models: model-router, gpt-image-1-mini, gpt-realtime
</context>

<tasks>

<task type="auto">
  <name>Task 1: Initialize FastAPI project with WebSocket</name>
  <files>backend/main.py, backend/requirements.txt</files>
  <action>
    Create backend directory and FastAPI application:
    1. Create backend/ directory
    2. Create requirements.txt with:
       - fastapi>=0.109.0
       - uvicorn[standard]>=0.27.0
       - websockets>=12.0
       - azure-identity>=1.15.0
       - azure-ai-inference>=1.0.0b1
       - python-dotenv>=1.0.0
    3. Create main.py with:
       - FastAPI app instance
       - CORS middleware allowing localhost:5173
       - GET / endpoint returning {"status": "ok", "service": "presto-changeo"}
       - WebSocket /ws endpoint that:
         - Accepts connection
         - Echoes received messages back (for testing)
         - Handles disconnection gracefully
       - Startup event logging server is running

    Use async def for WebSocket handler. Port 8000 (FastAPI default).
    Do NOT add complex message routing yet - just echo for connection testing.
  </action>
  <verify>
    cd backend && python -c "from main import app; print('FastAPI app loads')"
    # Should print without import errors
  </verify>
  <done>
    - backend/main.py exists with FastAPI app
    - WebSocket endpoint at /ws
    - CORS configured for frontend
    - requirements.txt has all dependencies
  </done>
</task>

<task type="auto">
  <name>Task 2: Create Azure authentication module</name>
  <files>backend/auth.py, .env.example</files>
  <action>
    Create Azure authentication utilities:
    1. Create backend/auth.py with:
       - get_azure_credential() -> returns InteractiveBrowserCredential instance
       - get_token(credential, scope: str) -> returns access token string
       - get_inference_client(credential) -> returns Azure AI Inference client configured for aipmaker-project
       - Lazy initialization (don't auth on import)
       - Error handling for auth failures
    2. Create .env.example in project root with:
       - AZURE_PROJECT_ENDPOINT=https://aipmaker-project-resource.services.ai.azure.com/api/projects/aipmaker-project
       - AZURE_DEPLOYMENT_NAME=model-router
       - Comments explaining each variable

    Use azure.identity.InteractiveBrowserCredential - this opens browser for user login.
    Do NOT hardcode credentials. Use environment variables loaded via python-dotenv.

    Note: get_inference_client should use azure.ai.inference.ChatCompletionsClient with the endpoint and credential.
  </action>
  <verify>
    cd backend && python -c "from auth import get_azure_credential; print('Auth module loads')"
    # Should print without import errors (actual auth not tested here)
  </verify>
  <done>
    - backend/auth.py exports get_azure_credential, get_token, get_inference_client
    - .env.example exists with Azure configuration template
    - Module imports without errors
  </done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] `cd backend && python -c "from main import app"` succeeds
- [ ] `cd backend && python -c "from auth import get_azure_credential"` succeeds
- [ ] backend/requirements.txt includes fastapi, uvicorn, azure-identity, azure-ai-inference
- [ ] .env.example exists with AZURE_ variables documented
</verification>

<success_criteria>

- Backend FastAPI project created with WebSocket endpoint
- Azure auth module ready for credential flow
- All Python imports work without errors
- Environment template documented
</success_criteria>

<output>
After completion, create `.planning/phases/01-foundation/01-02-SUMMARY.md`
</output>
