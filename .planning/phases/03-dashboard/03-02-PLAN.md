---
phase: 03-dashboard
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - backend/tools.py
  - backend/chat.py
autonomous: true

must_haves:
  truths:
    - "LLM receives tool definitions in system prompt"
    - "Tool calls are parsed from LLM response"
    - "Tool results are sent to frontend via WebSocket"
  artifacts:
    - path: "backend/tools.py"
      provides: "Tool definitions and execution"
      min_lines: 50
      exports: ["TOOL_DEFINITIONS", "execute_tool"]
  key_links:
    - from: "chat.py"
      to: "tools.py"
      via: "import TOOL_DEFINITIONS"
      pattern: "from tools import"
    - from: "chat.py"
      to: "websocket"
      via: "send tool_result message"
      pattern: "tool_result"
---

<objective>
Add LLM tool definitions for dashboard visualizations.

Purpose: Enable the AI to trigger visualizations by calling tools, which send structured data to the frontend for rendering.
Output: Backend tool infrastructure with show_chart and show_metrics tools.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md

# Prior plan context
@.planning/phases/02-chat-interface/02-01-SUMMARY.md

# Backend files
@backend/chat.py
@backend/main.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create tool definitions module</name>
  <files>backend/tools.py</files>
  <action>
Create backend/tools.py with tool definitions for the Azure OpenAI API.

Define two tools:
1. `show_chart` - Display a chart/visualization
   - Parameters: chart_type (string: "line", "bar", "pie", "area"), title (string), data (array of {label, value})

2. `show_metrics` - Update the metrics panel
   - Parameters: metrics (array of {label, value, unit})

Format tools for Azure OpenAI function calling:
```python
TOOL_DEFINITIONS = [
    {
        "type": "function",
        "function": {
            "name": "show_chart",
            "description": "Display a chart or visualization in the dashboard. Use this when the user asks to see data visually.",
            "parameters": {
                "type": "object",
                "properties": {
                    "chart_type": {
                        "type": "string",
                        "enum": ["line", "bar", "pie", "area"],
                        "description": "The type of chart to display"
                    },
                    "title": {
                        "type": "string",
                        "description": "Chart title"
                    },
                    "data": {
                        "type": "array",
                        "items": {
                            "type": "object",
                            "properties": {
                                "label": {"type": "string"},
                                "value": {"type": "number"}
                            },
                            "required": ["label", "value"]
                        },
                        "description": "Data points for the chart"
                    }
                },
                "required": ["chart_type", "title", "data"]
            }
        }
    },
    # ... show_metrics definition similarly
]
```

Also create an `execute_tool(name, arguments)` function that returns the tool result (for now, just return the arguments as-is since the "execution" is rendering on frontend).
  </action>
  <verify>python -c "from tools import TOOL_DEFINITIONS, execute_tool; print(len(TOOL_DEFINITIONS))" outputs 2</verify>
  <done>tools.py exports TOOL_DEFINITIONS list with 2 tools and execute_tool function</done>
</task>

<task type="auto">
  <name>Task 2: Integrate tools into chat handler</name>
  <files>backend/chat.py</files>
  <action>
Update chat.py to include tools in LLM calls and handle tool calls in responses.

Changes:
1. Import TOOL_DEFINITIONS and execute_tool from tools.py
2. Add tools parameter to the chat completion call
3. Handle tool_calls in the response:
   - When response contains tool_calls, iterate through them
   - Execute each tool via execute_tool()
   - Send tool_result message to frontend via WebSocket
   - Continue conversation with tool results (if needed)

Message protocol for tool results:
```python
await websocket.send_json({
    "type": "tool_result",
    "payload": {
        "tool": tool_name,
        "result": tool_result
    }
})
```

For streaming with tools:
- Azure OpenAI streams tool calls in chunks
- Accumulate tool call data until complete
- When tool call is complete, execute and send result

Handle the case where response has both content AND tool calls (send content as chat_chunk, then tool_result).

IMPORTANT: Do NOT break existing chat streaming - tool results are ADDITIONAL to the text response, not a replacement.
  </action>
  <verify>python -c "from chat import handle_chat_message; print('OK')" succeeds</verify>
  <done>Chat handler includes tools in LLM calls and sends tool_result messages via WebSocket</done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] `python -c "from tools import TOOL_DEFINITIONS"` succeeds
- [ ] `python -c "from chat import handle_chat_message"` succeeds
- [ ] TOOL_DEFINITIONS contains show_chart and show_metrics
- [ ] execute_tool function exists and returns result
- [ ] chat.py imports and uses tools
- [ ] No Python syntax errors
</verification>

<success_criteria>

- All tasks completed
- All verification checks pass
- Tools integrated into LLM conversation
- Tool results sent via WebSocket
  </success_criteria>

<output>
After completion, create `.planning/phases/03-dashboard/03-02-SUMMARY.md`
</output>
